# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kWhIk3V_8Uwp5JBepPjtUSYFUVEoOlAc

**Import Statements**
"""

import numpy as np
import pandas as pd
import tensorflow as tf

import re
import string
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Embedding
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping

from warnings import filterwarnings
filterwarnings('ignore')

"""**Load the dataset**"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
data = pd.read_csv('UpdatedResumeDataSet.csv')

data = pd.read_csv('./data/UpdatedResumeDataSet.csv')

data.head()

data['Category'].value_counts()

x_axis = data['Category'].value_counts().index.to_list()

y_axis = data['Category'].value_counts().to_list()



"""**Visualise the data**"""

def barPlot():
  fig, ax = plt.subplots(figsize =(16, 9))

  # Horizontal Bar Plot
  ax.barh(x_axis,y_axis)

  for s in ['top', 'bottom', 'left', 'right']:
      ax.spines[s].set_visible(False)

  # Remove x, y Ticks
  ax.xaxis.set_ticks_position('none')
  ax.yaxis.set_ticks_position('none')

  # Add padding between axes and labels
  ax.xaxis.set_tick_params(pad = 5)
  ax.yaxis.set_tick_params(pad = 10)

  # Add x, y gridlines
  ax.grid(b = True, color ='grey',
          linestyle ='-.', linewidth = 0.5,
          alpha = 0.2)

  # Show top values
  ax.invert_yaxis()

  # Add annotation to bars
  for i in ax.patches:
      plt.text(i.get_width()+0.2, i.get_y()+0.5,
              str(round((i.get_width()), 2)),
              fontsize = 10, fontweight ='bold',
              color ='grey')

  # Add Plot Title
  ax.set_title('Category',loc ='left', )

  # Add Text watermark
  fig.text(0.9, 0.15, 'Jeeteshgavande30', fontsize = 12,color ='grey', ha ='right', va ='bottom',alpha = 0.7)

  # Show Plot
  plt.show()

barPlot()

"""**Data Pre-processing**"""

def custom_standardization(Text):
    Text = re.sub('<br />', ' ', Text) #removing HTML
    Text = re.sub('https?://\S+|www\.\S+','', Text)#removing hyperlink
    Text = re.sub('RT|cc','', Text)# remove RT and cc
    Text = re.sub('#\S+','', Text) # remove hashtags

    Text = re.sub('\[.*?\]','', Text) #removing square brackets
    Text = re.sub('[^\x00-\x7f]','', Text)

    Text = re.sub( '[%s]' % re.escape(string.punctuation),'', Text) #removing puncuation
    Text = re.sub('\w*\d\w*','', Text)#remove words containing numbers
    return Text

data['CleanResume'] = data.Resume.apply(lambda x: custom_standardization(x))

data.head()

X = data['CleanResume']
y = data['Category']

# Replacing space between charecter(e.g "Data Science" to "DataScience")
# To get single array EX=> {Data:6},{Science:7},{HR:8}
# So we will have some array of 2D and some of 1D. That can cause the problem..
for i in range(len(y)):
  y[i] = y[i].replace(" ","")

"""**Split into test and train**"""

X_train, X_test, y_train,y_test = train_test_split(X,y, random_state=42, shuffle=True, test_size=0.17)

"""**Tokenize features and labels**"""

vocab_size = 6000
embedding_dim = 16
max_length = 6000
trunc_type='post'
padding_type='post'
oov_tok = "<OOV>"

"""**Tokenizing features -**"""

tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok, lower=True)
tokenizer.fit_on_texts(X)
word_index = tokenizer.word_index

X_sequences = tokenizer.texts_to_sequences(X_train)
X_padded = pad_sequences(X_sequences,maxlen=max_length,padding=padding_type, truncating=trunc_type)

testing_sequences = tokenizer.texts_to_sequences(X_test)
testing_padded = pad_sequences(testing_sequences,maxlen=max_length, padding=padding_type, truncating=trunc_type)

"""**Tokenizing labels -**"""

tokenizer = Tokenizer(lower=True)
tokenizer.fit_on_texts(y)
word_index = tokenizer.word_index

y_sequences = tokenizer.texts_to_sequences(y_train)
# y_padded = pad_sequences(y_sequences,maxlen=300,padding=padding_type, truncating=trunc_type)

y_testing_sequences = tokenizer.texts_to_sequences(y_test)
# y_testing_padded = pad_sequences(y_testing_sequences,maxlen=300, padding=padding_type, truncating=trunc_type)

y_sequences

"""**Building the model**"""

embedding_dim = 64
# input vocab of size 6000, and output embedding dimension of size 64

model = Sequential([
  Embedding(vocab_size, embedding_dim,),
  Bidirectional(LSTM(64)),
  Dense(64, activation='relu'),
  Dense(26, activation='softmax')# 26 lables are there....
])

model.summary()

# loss = tf.keras.losses.sparse_categorical_crossentropy(from_logits=False)
# loss = tf.keras.losses.categorical_crossentropy(from_logits=True)

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

y_padded = np.array(y_sequences)
y_testing_padded = np.array(y_testing_sequences)

earlyStopping = EarlyStopping(monitor='val_accuracy',patience=1)

history = model.fit(X_padded,y_padded,epochs = 10, validation_data=(testing_padded,y_testing_padded ), callbacks=[earlyStopping])

from google.colab import drive
drive.mount('/content/drive')

model.save('/content/drive/MyDrive/resume_classifier_model.h5')

"""**evaluating**"""

score = model.evaluate(testing_padded, y_testing_padded, verbose=1)

len(testing_padded)

len(y_testing_padded)

print("Test Score:", score[0])
print("Test Accuracy:", score[1])

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])

plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train','test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','test'], loc='upper left')
plt.show()

input_resume = input("Enter your resume content here: ")

cleaned_resume = custom_standardization(input_resume) # Apply the same preprocessing as before
input_sequence = tokenizer.texts_to_sequences([cleaned_resume]) # Tokenize
input_padded = pad_sequences(input_sequence, maxlen=max_length, padding=padding_type, truncating=trunc_type) # Pad

prediction = model.predict(input_padded)
predicted_category_index = np.argmax(prediction) # Get the index of the predicted category

# Get a list of all categories
categories = list(tokenizer.word_index.keys())
# Get predicted category using the index
predicted_category = categories[predicted_category_index-1]
# Print the prediction
print("Predicted Category:", predicted_category)